C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)
    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)
    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
ODEBlock(
  (odefunc): NODEfunc(
    (linear1): Linear(in_features=2, out_features=16, bias=True)
    (linear2): Linear(in_features=16, out_features=32, bias=True)
    (linear3): Linear(in_features=32, out_features=2, bias=True)
    (relu): ReLU(inplace)
  )
)
Number of parameters: 658
Epoch 0000 | Time 10.674 (10.674) | NFE-F 8000.0 | NFE-B 9000.0 | Train Err 0.0014 | Test Err 0.0014
Epoch 0001 | Time 16.961 (10.737) | NFE-F 8160.0 | NFE-B 9000.0 | Train Err 0.0013 | Test Err 0.0013
Epoch 0002 | Time 17.195 (10.801) | NFE-F 8318.4 | NFE-B 9000.0 | Train Err 0.0011 | Test Err 0.0011
Epoch 0003 | Time 16.289 (10.856) | NFE-F 8475.2 | NFE-B 9000.0 | Train Err 0.0010 | Test Err 0.0010
Epoch 0004 | Time 18.065 (10.928) | NFE-F 8630.5 | NFE-B 9000.0 | Train Err 0.0009 | Test Err 0.0009
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)
    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
Epoch 0005 | Time 16.988 (10.989) | NFE-F 8784.2 | NFE-B 9000.0 | Train Err 0.0009 | Test Err 0.0009
ODEBlock(
  (odefunc): NODEfunc(
    (linear1): Linear(in_features=2, out_features=16, bias=True)
    (linear2): Linear(in_features=16, out_features=32, bias=True)
    (linear3): Linear(in_features=32, out_features=2, bias=True)
    (relu): ReLU(inplace)
  )
)
Number of parameters: 658
Epoch 0006 | Time 22.659 (11.106) | NFE-F 8936.3 | NFE-B 9000.0 | Train Err 0.0010 | Test Err 0.0010
Epoch 0000 | Time 15.353 (15.353) | NFE-F 8000.0 | NFE-B 9000.0 | Train Err 0.0018 | Test Err 0.0018
Epoch 0007 | Time 22.176 (11.216) | NFE-F 9087.0 | NFE-B 9000.0 | Train Err 0.0010 | Test Err 0.0010
Epoch 0001 | Time 22.202 (15.422) | NFE-F 8160.0 | NFE-B 9000.0 | Train Err 0.0015 | Test Err 0.0015
Epoch 0008 | Time 21.624 (11.320) | NFE-F 9236.1 | NFE-B 9000.0 | Train Err 0.0010 | Test Err 0.0010
Epoch 0002 | Time 21.466 (15.482) | NFE-F 8318.4 | NFE-B 9000.0 | Train Err 0.0013 | Test Err 0.0013
Epoch 0009 | Time 29.828 (11.505) | NFE-F 9383.7 | NFE-B 9000.0 | Train Err 0.0010 | Test Err 0.0010
Epoch 0003 | Time 29.814 (15.625) | NFE-F 8475.2 | NFE-B 9000.0 | Train Err 0.0011 | Test Err 0.0011
Epoch 0004 | Time 22.345 (15.692) | NFE-F 8630.5 | NFE-B 9000.0 | Train Err 0.0010 | Test Err 0.0010
Epoch 0010 | Time 22.497 (11.615) | NFE-F 9529.9 | NFE-B 9000.0 | Train Err 0.0010 | Test Err 0.0010
Epoch 0005 | Time 24.988 (15.785) | NFE-F 8784.2 | NFE-B 9000.0 | Train Err 0.0010 | Test Err 0.0010
Epoch 0011 | Time 24.972 (11.749) | NFE-F 9674.6 | NFE-B 9000.0 | Train Err 0.0009 | Test Err 0.0009
Epoch 0006 | Time 22.209 (15.850) | NFE-F 8936.3 | NFE-B 9000.0 | Train Err 0.0010 | Test Err 0.0010
Epoch 0012 | Time 22.349 (11.855) | NFE-F 9817.8 | NFE-B 9000.0 | Train Err 0.0009 | Test Err 0.0009
Epoch 0007 | Time 19.702 (15.888) | NFE-F 9087.0 | NFE-B 9000.0 | Train Err 0.0010 | Test Err 0.0010
Epoch 0013 | Time 19.791 (11.934) | NFE-F 9959.7 | NFE-B 9000.0 | Train Err 0.0008 | Test Err 0.0008
Epoch 0008 | Time 20.790 (15.937) | NFE-F 9236.1 | NFE-B 9000.0 | Train Err 0.0011 | Test Err 0.0011
Epoch 0014 | Time 20.720 (12.022) | NFE-F 10100.1 | NFE-B 9000.0 | Train Err 0.0008 | Test Err 0.0008
Epoch 0009 | Time 19.155 (15.969) | NFE-F 9383.7 | NFE-B 9000.0 | Train Err 0.0012 | Test Err 0.0012
Epoch 0015 | Time 19.188 (12.094) | NFE-F 10239.1 | NFE-B 9000.0 | Train Err 0.0008 | Test Err 0.0008
Epoch 0010 | Time 19.455 (16.004) | NFE-F 9529.9 | NFE-B 9000.0 | Train Err 0.0012 | Test Err 0.0012
Epoch 0016 | Time 19.411 (12.167) | NFE-F 10376.7 | NFE-B 9000.0 | Train Err 0.0008 | Test Err 0.0008
Epoch 0011 | Time 19.837 (16.043) | NFE-F 9674.6 | NFE-B 9000.0 | Train Err 0.0011 | Test Err 0.0011
Epoch 0017 | Time 19.912 (12.244) | NFE-F 10512.9 | NFE-B 9000.0 | Train Err 0.0008 | Test Err 0.0008
Epoch 0012 | Time 19.844 (16.081) | NFE-F 9817.8 | NFE-B 9000.0 | Train Err 0.0010 | Test Err 0.0010
Epoch 0018 | Time 19.976 (12.322) | NFE-F 10647.8 | NFE-B 9000.0 | Train Err 0.0008 | Test Err 0.0008
Epoch 0013 | Time 18.845 (16.108) | NFE-F 9959.7 | NFE-B 9000.0 | Train Err 0.0010 | Test Err 0.0010
Epoch 0019 | Time 19.087 (12.389) | NFE-F 10781.3 | NFE-B 9000.0 | Train Err 0.0007 | Test Err 0.0007
Epoch 0014 | Time 18.775 (16.135) | NFE-F 10100.1 | NFE-B 9000.0 | Train Err 0.0009 | Test Err 0.0009
Epoch 0020 | Time 18.661 (12.452) | NFE-F 10913.5 | NFE-B 9000.0 | Train Err 0.0007 | Test Err 0.0007
Epoch 0015 | Time 18.935 (16.163) | NFE-F 10239.1 | NFE-B 9000.0 | Train Err 0.0008 | Test Err 0.0008
Epoch 0021 | Time 19.133 (12.519) | NFE-F 11044.4 | NFE-B 9000.0 | Train Err 0.0007 | Test Err 0.0007
Epoch 0016 | Time 18.932 (16.191) | NFE-F 10376.7 | NFE-B 9000.0 | Train Err 0.0008 | Test Err 0.0008
Epoch 0022 | Time 18.901 (12.583) | NFE-F 11173.9 | NFE-B 9000.0 | Train Err 0.0007 | Test Err 0.0007
Epoch 0017 | Time 27.497 (16.304) | NFE-F 10512.9 | NFE-B 9000.0 | Train Err 0.0008 | Test Err 0.0008
Epoch 0023 | Time 27.522 (12.732) | NFE-F 11302.2 | NFE-B 9000.0 | Train Err 0.0007 | Test Err 0.0007
Epoch 0018 | Time 24.572 (16.386) | NFE-F 10647.8 | NFE-B 9000.0 | Train Err 0.0008 | Test Err 0.0008
Epoch 0024 | Time 24.482 (12.850) | NFE-F 11429.1 | NFE-B 9000.0 | Train Err 0.0006 | Test Err 0.0006
Epoch 0019 | Time 20.195 (16.424) | NFE-F 10781.3 | NFE-B 9000.0 | Train Err 0.0008 | Test Err 0.0008
Epoch 0025 | Time 20.425 (12.925) | NFE-F 11554.9 | NFE-B 9000.0 | Train Err 0.0006 | Test Err 0.0006
Epoch 0020 | Time 24.969 (16.510) | NFE-F 10913.5 | NFE-B 9000.0 | Train Err 0.0008 | Test Err 0.0008
Epoch 0026 | Time 24.866 (13.045) | NFE-F 11679.3 | NFE-B 9000.0 | Train Err 0.0006 | Test Err 0.0006
Epoch 0021 | Time 22.262 (16.567) | NFE-F 11044.4 | NFE-B 9000.0 | Train Err 0.0008 | Test Err 0.0008
Epoch 0027 | Time 22.523 (13.140) | NFE-F 11802.5 | NFE-B 9000.0 | Train Err 0.0006 | Test Err 0.0006
Epoch 0022 | Time 23.693 (16.639) | NFE-F 11173.9 | NFE-B 9000.0 | Train Err 0.0008 | Test Err 0.0008
Epoch 0028 | Time 23.661 (13.245) | NFE-F 11924.5 | NFE-B 9000.0 | Train Err 0.0006 | Test Err 0.0006
Epoch 0023 | Time 26.513 (16.737) | NFE-F 11302.2 | NFE-B 9000.0 | Train Err 0.0008 | Test Err 0.0008
Epoch 0029 | Time 27.029 (13.383) | NFE-F 12045.2 | NFE-B 9000.0 | Train Err 0.0006 | Test Err 0.0006
Epoch 0024 | Time 20.603 (16.776) | NFE-F 11429.1 | NFE-B 9000.0 | Train Err 0.0008 | Test Err 0.0008
Epoch 0030 | Time 19.739 (13.446) | NFE-F 12164.8 | NFE-B 9000.0 | Train Err 0.0006 | Test Err 0.0006
Epoch 0025 | Time 21.309 (16.821) | NFE-F 11554.9 | NFE-B 9000.0 | Train Err 0.0007 | Test Err 0.0007
Epoch 0031 | Time 21.726 (13.529) | NFE-F 12283.1 | NFE-B 9000.0 | Train Err 0.0006 | Test Err 0.0006
Epoch 0026 | Time 21.010 (16.863) | NFE-F 11679.3 | NFE-B 9000.0 | Train Err 0.0007 | Test Err 0.0007
Epoch 0032 | Time 20.701 (13.601) | NFE-F 12400.3 | NFE-B 9000.0 | Train Err 0.0005 | Test Err 0.0005
Epoch 0027 | Time 21.395 (16.909) | NFE-F 11802.5 | NFE-B 9000.0 | Train Err 0.0007 | Test Err 0.0007
Epoch 0033 | Time 21.430 (13.679) | NFE-F 12516.3 | NFE-B 9000.0 | Train Err 0.0005 | Test Err 0.0005
Epoch 0028 | Time 21.161 (16.951) | NFE-F 11924.5 | NFE-B 9000.0 | Train Err 0.0007 | Test Err 0.0007
Epoch 0034 | Time 22.304 (13.765) | NFE-F 12631.1 | NFE-B 9000.0 | Train Err 0.0005 | Test Err 0.0005
Epoch 0029 | Time 24.611 (17.028) | NFE-F 12045.2 | NFE-B 9000.0 | Train Err 0.0007 | Test Err 0.0007
Epoch 0035 | Time 23.678 (13.864) | NFE-F 12744.8 | NFE-B 9000.0 | Train Err 0.0005 | Test Err 0.0005
Epoch 0030 | Time 22.462 (17.082) | NFE-F 12164.8 | NFE-B 9000.0 | Train Err 0.0006 | Test Err 0.0006
Epoch 0036 | Time 22.402 (13.950) | NFE-F 12857.4 | NFE-B 9000.0 | Train Err 0.0005 | Test Err 0.0005
Epoch 0031 | Time 21.413 (17.125) | NFE-F 12283.1 | NFE-B 9000.0 | Train Err 0.0006 | Test Err 0.0006
Epoch 0037 | Time 21.434 (14.025) | NFE-F 12968.8 | NFE-B 9000.0 | Train Err 0.0005 | Test Err 0.0005
Epoch 0032 | Time 20.830 (17.162) | NFE-F 12400.3 | NFE-B 9000.0 | Train Err 0.0006 | Test Err 0.0006
Epoch 0038 | Time 20.718 (14.092) | NFE-F 13079.1 | NFE-B 9000.0 | Train Err 0.0005 | Test Err 0.0005
Epoch 0033 | Time 20.531 (17.196) | NFE-F 12516.3 | NFE-B 9000.0 | Train Err 0.0006 | Test Err 0.0006
Epoch 0039 | Time 20.757 (14.158) | NFE-F 13188.3 | NFE-B 9000.0 | Train Err 0.0005 | Test Err 0.0005
Epoch 0034 | Time 22.674 (17.251) | NFE-F 12631.1 | NFE-B 9000.0 | Train Err 0.0006 | Test Err 0.0006
Epoch 0040 | Time 22.939 (14.246) | NFE-F 13296.5 | NFE-B 9000.0 | Train Err 0.0005 | Test Err 0.0005
Epoch 0035 | Time 21.420 (17.293) | NFE-F 12744.8 | NFE-B 9000.0 | Train Err 0.0006 | Test Err 0.0006
Epoch 0041 | Time 21.393 (14.317) | NFE-F 13403.5 | NFE-B 9000.0 | Train Err 0.0005 | Test Err 0.0005
Epoch 0036 | Time 18.790 (17.308) | NFE-F 12857.4 | NFE-B 9000.0 | Train Err 0.0006 | Test Err 0.0006
Epoch 0042 | Time 18.879 (14.363) | NFE-F 13509.5 | NFE-B 9000.0 | Train Err 0.0005 | Test Err 0.0005
Epoch 0037 | Time 26.316 (17.398) | NFE-F 12968.8 | NFE-B 9000.0 | Train Err 0.0006 | Test Err 0.0006
Epoch 0043 | Time 26.739 (14.487) | NFE-F 13614.4 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0038 | Time 26.450 (17.488) | NFE-F 13079.1 | NFE-B 9000.0 | Train Err 0.0006 | Test Err 0.0006
Epoch 0044 | Time 26.075 (14.603) | NFE-F 13718.2 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0039 | Time 25.100 (17.564) | NFE-F 13188.3 | NFE-B 9000.0 | Train Err 0.0005 | Test Err 0.0005
Epoch 0045 | Time 25.085 (14.708) | NFE-F 13821.0 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0040 | Time 25.272 (17.641) | NFE-F 13296.5 | NFE-B 9000.0 | Train Err 0.0005 | Test Err 0.0005
Epoch 0046 | Time 26.661 (14.827) | NFE-F 13922.8 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0041 | Time 26.983 (17.735) | NFE-F 13403.5 | NFE-B 9000.0 | Train Err 0.0005 | Test Err 0.0005
Epoch 0047 | Time 25.704 (14.936) | NFE-F 14023.6 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0042 | Time 24.262 (17.800) | NFE-F 13509.5 | NFE-B 9000.0 | Train Err 0.0005 | Test Err 0.0005
Epoch 0048 | Time 23.936 (15.026) | NFE-F 14123.4 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0043 | Time 20.299 (17.825) | NFE-F 13614.4 | NFE-B 9000.0 | Train Err 0.0005 | Test Err 0.0005
Epoch 0049 | Time 20.541 (15.081) | NFE-F 14222.1 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0044 | Time 23.614 (17.883) | NFE-F 13718.2 | NFE-B 9000.0 | Train Err 0.0005 | Test Err 0.0005
Epoch 0050 | Time 24.067 (15.171) | NFE-F 14319.9 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0045 | Time 31.963 (18.024) | NFE-F 13821.0 | NFE-B 9000.0 | Train Err 0.0005 | Test Err 0.0005
Epoch 0051 | Time 32.650 (15.346) | NFE-F 14416.7 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0046 | Time 26.637 (18.110) | NFE-F 13922.8 | NFE-B 9000.0 | Train Err 0.0005 | Test Err 0.0005
Epoch 0052 | Time 25.483 (15.447) | NFE-F 14512.5 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0047 | Time 23.465 (18.163) | NFE-F 14023.6 | NFE-B 9000.0 | Train Err 0.0005 | Test Err 0.0005
Epoch 0053 | Time 23.692 (15.529) | NFE-F 14607.4 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0048 | Time 26.834 (18.250) | NFE-F 14123.4 | NFE-B 9000.0 | Train Err 0.0005 | Test Err 0.0005
Epoch 0054 | Time 27.017 (15.644) | NFE-F 14701.3 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0049 | Time 25.299 (18.321) | NFE-F 14222.1 | NFE-B 9000.0 | Train Err 0.0005 | Test Err 0.0005
Epoch 0055 | Time 26.168 (15.750) | NFE-F 14794.3 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0050 | Time 26.237 (18.400) | NFE-F 14319.9 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0056 | Time 24.898 (15.841) | NFE-F 14886.4 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0051 | Time 22.109 (18.437) | NFE-F 14416.7 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0057 | Time 22.282 (15.905) | NFE-F 14977.5 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0052 | Time 21.289 (18.465) | NFE-F 14512.5 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0058 | Time 21.242 (15.959) | NFE-F 15067.7 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0053 | Time 22.900 (18.510) | NFE-F 14607.4 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0059 | Time 23.238 (16.032) | NFE-F 15157.1 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0054 | Time 22.800 (18.553) | NFE-F 14701.3 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0060 | Time 22.921 (16.101) | NFE-F 15245.5 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0055 | Time 23.093 (18.598) | NFE-F 14794.3 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0061 | Time 22.837 (16.168) | NFE-F 15333.0 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0056 | Time 21.766 (18.630) | NFE-F 14886.4 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0062 | Time 21.681 (16.223) | NFE-F 15419.7 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0057 | Time 20.881 (18.652) | NFE-F 14977.5 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0063 | Time 22.066 (16.281) | NFE-F 15505.5 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0058 | Time 28.760 (18.753) | NFE-F 15067.7 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0064 | Time 28.356 (16.402) | NFE-F 15590.5 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0059 | Time 27.370 (18.839) | NFE-F 15157.1 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0065 | Time 27.189 (16.510) | NFE-F 15674.6 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0060 | Time 25.785 (18.909) | NFE-F 15245.5 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0066 | Time 25.276 (16.598) | NFE-F 15757.8 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0061 | Time 25.169 (18.972) | NFE-F 15333.0 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0067 | Time 25.449 (16.686) | NFE-F 15840.2 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0062 | Time 22.542 (19.007) | NFE-F 15419.7 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0068 | Time 22.875 (16.748) | NFE-F 15921.8 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0063 | Time 23.764 (19.055) | NFE-F 15505.5 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0069 | Time 23.416 (16.815) | NFE-F 16002.6 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0064 | Time 25.057 (19.115) | NFE-F 15590.5 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0070 | Time 25.518 (16.902) | NFE-F 16082.6 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0065 | Time 25.345 (19.177) | NFE-F 15674.6 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0071 | Time 25.286 (16.986) | NFE-F 16161.8 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0066 | Time 21.603 (19.201) | NFE-F 15757.8 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0072 | Time 22.006 (17.036) | NFE-F 16240.1 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0067 | Time 24.359 (19.253) | NFE-F 15840.2 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0073 | Time 24.209 (17.108) | NFE-F 16317.7 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0068 | Time 22.497 (19.285) | NFE-F 15921.8 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0074 | Time 22.467 (17.161) | NFE-F 16394.6 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0069 | Time 20.611 (19.299) | NFE-F 16002.6 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0075 | Time 20.151 (17.191) | NFE-F 16470.6 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0070 | Time 19.393 (19.300) | NFE-F 16082.6 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0076 | Time 19.590 (17.215) | NFE-F 16545.9 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0071 | Time 18.888 (19.295) | NFE-F 16161.8 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0077 | Time 19.047 (17.233) | NFE-F 16620.4 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0072 | Time 19.771 (19.300) | NFE-F 16240.1 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0078 | Time 19.557 (17.257) | NFE-F 16694.2 | NFE-B 9000.0 | Train Err 0.0003 | Test Err 0.0003
Epoch 0073 | Time 19.778 (19.305) | NFE-F 16317.7 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0079 | Time 19.945 (17.284) | NFE-F 16767.3 | NFE-B 9000.0 | Train Err 0.0003 | Test Err 0.0003
Epoch 0074 | Time 18.998 (19.302) | NFE-F 16394.6 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0080 | Time 19.337 (17.304) | NFE-F 16839.6 | NFE-B 9000.0 | Train Err 0.0003 | Test Err 0.0003
Epoch 0075 | Time 19.365 (19.303) | NFE-F 16470.6 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0081 | Time 19.047 (17.321) | NFE-F 16911.2 | NFE-B 9000.0 | Train Err 0.0003 | Test Err 0.0003
Epoch 0076 | Time 19.757 (19.307) | NFE-F 16545.9 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0082 | Time 19.849 (17.347) | NFE-F 16982.1 | NFE-B 9000.0 | Train Err 0.0003 | Test Err 0.0003
Epoch 0077 | Time 18.809 (19.302) | NFE-F 16620.4 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0083 | Time 19.065 (17.364) | NFE-F 17052.3 | NFE-B 9000.0 | Train Err 0.0003 | Test Err 0.0003
Epoch 0078 | Time 19.102 (19.300) | NFE-F 16694.2 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0084 | Time 19.316 (17.383) | NFE-F 17121.8 | NFE-B 9000.0 | Train Err 0.0003 | Test Err 0.0003
Epoch 0079 | Time 18.471 (19.292) | NFE-F 16767.3 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0085 | Time 18.633 (17.396) | NFE-F 17190.6 | NFE-B 9000.0 | Train Err 0.0003 | Test Err 0.0003
Epoch 0080 | Time 19.045 (19.289) | NFE-F 16839.6 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0086 | Time 19.165 (17.414) | NFE-F 17258.7 | NFE-B 9000.0 | Train Err 0.0003 | Test Err 0.0003
Epoch 0081 | Time 18.682 (19.283) | NFE-F 16911.2 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0087 | Time 18.550 (17.425) | NFE-F 17326.1 | NFE-B 9000.0 | Train Err 0.0003 | Test Err 0.0003
Epoch 0082 | Time 18.787 (19.278) | NFE-F 16982.1 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0088 | Time 19.466 (17.445) | NFE-F 17392.8 | NFE-B 9000.0 | Train Err 0.0003 | Test Err 0.0003
Epoch 0083 | Time 19.703 (19.283) | NFE-F 17052.3 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0089 | Time 19.189 (17.463) | NFE-F 17458.9 | NFE-B 9000.0 | Train Err 0.0003 | Test Err 0.0003
Epoch 0084 | Time 19.519 (19.285) | NFE-F 17121.8 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0090 | Time 19.651 (17.485) | NFE-F 17524.3 | NFE-B 9000.0 | Train Err 0.0003 | Test Err 0.0003
Epoch 0085 | Time 19.065 (19.283) | NFE-F 17190.6 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0091 | Time 19.199 (17.502) | NFE-F 17589.0 | NFE-B 9000.0 | Train Err 0.0003 | Test Err 0.0003
Epoch 0086 | Time 19.659 (19.287) | NFE-F 17258.7 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0092 | Time 19.620 (17.523) | NFE-F 17653.2 | NFE-B 9000.0 | Train Err 0.0003 | Test Err 0.0003
Epoch 0087 | Time 19.620 (19.290) | NFE-F 17326.1 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0093 | Time 19.913 (17.547) | NFE-F 17716.6 | NFE-B 9000.0 | Train Err 0.0003 | Test Err 0.0003
Epoch 0088 | Time 19.458 (19.292) | NFE-F 17392.8 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0094 | Time 19.844 (17.570) | NFE-F 17779.5 | NFE-B 9000.0 | Train Err 0.0003 | Test Err 0.0003
Epoch 0089 | Time 20.247 (19.301) | NFE-F 17458.9 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0095 | Time 20.320 (17.597) | NFE-F 17841.7 | NFE-B 9000.0 | Train Err 0.0003 | Test Err 0.0003
Epoch 0090 | Time 20.444 (19.313) | NFE-F 17524.3 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0096 | Time 20.563 (17.627) | NFE-F 17903.2 | NFE-B 9000.0 | Train Err 0.0003 | Test Err 0.0003
Epoch 0091 | Time 20.164 (19.321) | NFE-F 17589.0 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0097 | Time 20.329 (17.654) | NFE-F 17964.2 | NFE-B 9000.0 | Train Err 0.0003 | Test Err 0.0003
Epoch 0092 | Time 20.087 (19.329) | NFE-F 17653.2 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0098 | Time 20.131 (17.679) | NFE-F 18024.6 | NFE-B 9000.0 | Train Err 0.0003 | Test Err 0.0003
Epoch 0093 | Time 20.040 (19.336) | NFE-F 17716.6 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0099 | Time 20.422 (17.706) | NFE-F 18084.3 | NFE-B 9000.0 | Train Err 0.0003 | Test Err 0.0003
Epoch 0094 | Time 20.472 (19.347) | NFE-F 17779.5 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0095 | Time 19.638 (19.350) | NFE-F 17841.7 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0096 | Time 25.711 (19.414) | NFE-F 17903.2 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0097 | Time 21.935 (19.439) | NFE-F 17964.2 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0098 | Time 21.972 (19.464) | NFE-F 18024.6 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
Epoch 0099 | Time 20.358 (19.473) | NFE-F 18084.3 | NFE-B 9000.0 | Train Err 0.0004 | Test Err 0.0004
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)
    device = torch.device('cuda:' + str(args.gpu))# if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)
    device = torch.device('cuda:' + str(args.gpu))# if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)
    device = torch.device('cuda:' + str(args.gpu))# if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)
    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)
    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)
    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)
    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
ODEBlock(
  (odefunc): NODEfunc(
    (linear1): Linear(in_features=2, out_features=16, bias=True)
    (linear2): Linear(in_features=16, out_features=32, bias=True)
    (linear3): Linear(in_features=32, out_features=2, bias=True)
    (relu): ReLU(inplace)
  )
)
Number of parameters: 658
Epoch 0000 | Time 9.999 (9.999) | NFE-F 8000.0 | NFE-B 9000.0 | Train Err 0.0023 | Test Err 0.0023
Epoch 0001 | Time 16.278 (10.062) | NFE-F 8160.0 | NFE-B 9000.0 | Train Err 0.0018 | Test Err 0.0018
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)
    device = torch.cuda.device(0)
    #device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)
    device = torch.cuda.device(1)
    #device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)
    device = torch.cuda.device(2)
    #device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)
    device = torch.cuda.device('cuda:0')
    #device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)
    device = torch.cuda.device('cuda:2')
    #device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)
    device = torch.cuda.device('cuda:1')
    #device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"]= '0'  # specify which GPU(s) to be used
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)
    #device = torch.cuda.device('cuda:1')
    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
ODEBlock(
  (odefunc): NODEfunc(
    (linear1): Linear(in_features=2, out_features=16, bias=True)
    (linear2): Linear(in_features=16, out_features=32, bias=True)
    (linear3): Linear(in_features=32, out_features=2, bias=True)
    (relu): ReLU(inplace)
  )
)
Number of parameters: 658
Epoch 0000 | Time 9.986 (9.986) | NFE-F 8000.0 | NFE-B 9000.0 | Train Err 0.0018 | Test Err 0.0018
Epoch 0001 | Time 17.123 (10.058) | NFE-F 8160.0 | NFE-B 9000.0 | Train Err 0.0015 | Test Err 0.0015
Epoch 0002 | Time 16.865 (10.126) | NFE-F 8318.4 | NFE-B 9000.0 | Train Err 0.0012 | Test Err 0.0012
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"]= '0'  # specify which GPU(s) to be used
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)
    device = torch.cuda.device('cuda')
    #device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"]= '0'  # specify which GPU(s) to be used
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)
    device = torch.device('cuda')
    #device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"]= '0'  # specify which GPU(s) to be used
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)
    device = torch.device('cuda:0')
    #device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
#os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
#os.environ["CUDA_VISIBLE_DEVICES"]= '0'  # specify which GPU(s) to be used
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)
    device = torch.cuda.set_device(0)
    #device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
#os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
#os.environ["CUDA_VISIBLE_DEVICES"]= '0'  # specify which GPU(s) to be used
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)
    device = torch._C._cuda_setDevice(-1)

    #device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
#os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
#os.environ["CUDA_VISIBLE_DEVICES"]= '0'  # specify which GPU(s) to be used
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)
    device = torch._C._cuda_setDevice(0)

    #device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
#os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
#os.environ["CUDA_VISIBLE_DEVICES"]= '0'  # specify which GPU(s) to be used
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device(0)#'cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
#os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
#os.environ["CUDA_VISIBLE_DEVICES"]= '0'  # specify which GPU(s) to be used
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device(0)#'cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
#os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
#os.environ["CUDA_VISIBLE_DEVICES"]= '0'  # specify which GPU(s) to be used
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device(0)#'cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
#os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
#os.environ["CUDA_VISIBLE_DEVICES"]= '0'  # specify which GPU(s) to be used
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:0')# + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
#os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
#os.environ["CUDA_VISIBLE_DEVICES"]= '0'  # specify which GPU(s) to be used
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
#os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
#os.environ["CUDA_VISIBLE_DEVICES"]= '0'  # specify which GPU(s) to be used
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
#os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
#os.environ["CUDA_VISIBLE_DEVICES"]= '0'  # specify which GPU(s) to be used
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock
torch.set_default_tensor_type('torch.cuda.FloatTensor')

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
ODEBlock(
  (odefunc): NODEfunc(
    (linear1): Linear(in_features=2, out_features=16, bias=True)
    (linear2): Linear(in_features=16, out_features=32, bias=True)
    (linear3): Linear(in_features=32, out_features=2, bias=True)
    (relu): ReLU(inplace=True)
  )
)
Number of parameters: 658
Epoch 0000 | Time 84.579 (84.579) | NFE-F 8000.0 | NFE-B 9000.0 | Train Err 0.0009 | Test Err 0.0009
Epoch 0001 | Time 122.553 (84.959) | NFE-F 8160.0 | NFE-B 9000.0 | Train Err 0.0008 | Test Err 0.0008
Epoch 0002 | Time 120.518 (85.314) | NFE-F 8318.4 | NFE-B 9000.0 | Train Err 0.0008 | Test Err 0.0008
Epoch 0003 | Time 119.467 (85.656) | NFE-F 8475.2 | NFE-B 9000.0 | Train Err 0.0007 | Test Err 0.0007
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
#os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
#os.environ["CUDA_VISIBLE_DEVICES"]= '0'  # specify which GPU(s) to be used
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock
#torch.set_default_tensor_type('torch.cuda.FloatTensor')

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
C:\Users\harsh\PycharmProjects\SimML\experiment.py
# TODO: develop and validate models for these cases
# TODO: visualize 2d data on toy cases (stochastic, hybrid)
# TODO: develop and validate models for these cases

import os
#os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
#os.environ["CUDA_VISIBLE_DEVICES"]= '0'  # specify which GPU(s) to be used
import argparse
import time
import numpy as np
import torch
from torch import nn
import utils
from utils import RunningAverageMeter, accuracy
from simulations import SwitchDataset
import matplotlib
matplotlib.use("TkAgg")
from models import NODEfunc, ODEBlock
torch.set_default_tensor_type('torch.cuda.FloatTensor')

parser = argparse.ArgumentParser()
parser.add_argument('--physics', type=str, choices=['switch'], default='switch')  # choose physics model
parser.add_argument('--time_span', type=float, default=1.)  # time span for simulation
parser.add_argument('--dt', type=float, default=0.001)  # time step for simulation
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')  # choose surrogate model
parser.add_argument('--tol', type=float, default=1e-3)  # tolerance for ode solver
parser.add_argument('--adjoint', type=eval, default=True, choices=[True, False])  # method for computing gradient
parser.add_argument('--nepochs', type=int, default=100)  # number of training epochs
parser.add_argument('--lr', type=float, default=0.1)  # learning rate
parser.add_argument('--batch_size', type=int, default=20)  # batch size for training
parser.add_argument('--test_batch_size', type=int, default=20)  # batch size for validation and test
parser.add_argument('--save', type=str, default='./experiment1')  # save dir
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()


if __name__ == '__main__':

    utils.makedirs(args.save)
    logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    # visualize the model
    def create_dataset(model_type):
        # create simulation dataset
        return {
            # add new models here
            'switch': SwitchDataset(device, args.time_span, args.test_batch_size, args.dt, args.tol, args.tol)
        }[model_type]

    physics_simulation = create_dataset(args.physics)
    # vis = utils.Visualization(dataset=physics_simulation)
    # vis.plot_data()

    # train a baseline Neural ODE model
    # NOTE: please distinguish between physical and surrogate (statistical) models
    settings = {'odefunc': NODEfunc(2),
                'device': device,
                'rtol': args.tol,
                'atol': args.tol}
    surrogate_model = ODEBlock(settings).to(device)

    # save model info
    logger.info(surrogate_model)
    logger.info('Number of parameters: {}'.format(utils.count_parameters(surrogate_model)))

    # define loss
    criterion = nn.MSELoss().to(device)  # TODO: check loss definition

    # get data streamer
    train_loader, test_loader, train_eval_loader = utils.get_data_loaders(physics_simulation, args.batch_size, args.test_batch_size)
    data_gen = utils.inf_generator(train_loader)
    batches_per_epoch = int(args.test_batch_size / args.batch_size)

    # training process
#####################################################################
    # TODO: need to fine-tune the learning rate scheme?
    lr_fn = utils.learning_rate_with_decay(
        args.lr, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001]
    )

    # set up the optimizer
    optimizer = torch.optim.SGD(surrogate_model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = utils.RunningAverageMeter()
    f_nfe_meter = utils.RunningAverageMeter()
    b_nfe_meter = utils.RunningAverageMeter()
    end = time.time()

    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x = data_gen.__next__()
        x0 = x[:,:,0]  # get the initial states
        x0 = x0.to(device)
        pred = surrogate_model(x0.float(), args.dt)  # output is batch_size * dim * time, excluding the initial state
        # compute MSE between physical and surrogate model
        loss = criterion(pred, x[:,:,1:].float())  # exclude initial states since they are the same for pred and target

        # monitor forward ODE steps
        nfe_forward = surrogate_model.nfe
        surrogate_model.nfe = 0

        # compute gradient and do gradient descent
        loss.backward()
        optimizer.step()

        # monitor adjoint steps
        nfe_backward = surrogate_model.nfe
        surrogate_model.nfe = 0

        batch_time_meter.update(time.time() - end)

        f_nfe_meter.update(nfe_forward)
        b_nfe_meter.update(nfe_backward)

        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(surrogate_model, args.dt, criterion, train_eval_loader, device)
                val_acc = accuracy(surrogate_model, args.dt, criterion, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': surrogate_model.state_dict(), 'args': args},
                               os.path.join(args.save, 'surrogate_model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Err {:.4f} | Test Err {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
#####################################################################

    # visualize surrogate model along with the physics simulation

    # # TODO: load the model if pre-trained
    # surrogate_model.load_state_dict(torch.load(os.path.join(args.save, 'surrogate_model.pth'))['state_dict'],
    #                                 strict=False)
    # surrogate_model.eval()

    dim, time_step = physics_simulation.dim()
    predict = np.empty((0, dim, time_step))
    for x in test_loader:
        target = x[:,:,1:].to(device).float()
        x0 = x[:,:,0].to(device)
        x = surrogate_model(x0.float(), args.dt).cpu().detach().numpy()
        x0 = np.expand_dims(x0.cpu().detach().numpy(), axis=2)
        x = np.concatenate((x0, x), axis=2)  # add the initial state back in
        predict = np.concatenate((predict, x), axis=0)

    vis = utils.Visualization(dataset=[physics_simulation, predict])
    vis.compare_data()

Namespace(adjoint=True, batch_size=20, debug=False, dt=0.001, gpu=0, lr=0.1, nepochs=100, network='odenet', physics='switch', save='./experiment1', test_batch_size=20, time_span=1.0, tol=0.001)
